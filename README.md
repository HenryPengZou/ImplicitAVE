# ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction

![Task](pipeline.jpg)

This repository contains the dataset and code of the paper:
> **ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction** 
> [[Paper]]() [[ACL Anthology]]() [[OpenReview]]() [[arXiv]]() <br>
> Under Review <br>
> Henry Peng Zou, Vinay Samuel, Yue Zhou, Weizhi Zhang, Liancheng Fang, Zihe Song, Philip S. Yu, Cornelia Caragea <br>


## Datasets
We are still building and updaing this repository. Please stay tuned! But feel free to contact me (pzou3@uic.edu) if you need the dataset or code urgently.

## Bugs or Questions

If you have any questions related to the dataset or the paper, feel free to email Henry Peng Zou (pzou3@uic.edu). If you encounter any problems when using the code, or want to report a bug, you can open an issue. Please try to specify the problem with details so we can help you better and quicker!


## Citation
If you find this repository helpful, please consider citing our paper ðŸ’•: 
```bibtex
@inproceedings{zou2024implicitave,
    title={ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction},
    author={Henry Peng Zou, Vinay Samuel, Yue Zhou, Weizhi Zhang, Liancheng Fang, Zihe Song, Philip S. Yu, Cornelia Caragea},
    booktitle={ArXiv; Under Review},
    year={2024}
}

@inproceedings{zou2024eiven,
    title={EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM},
    author={Henry Peng Zou, Gavin Heqing Yu, Ziwei Fan, Dan Bu, Han Liu, Peng Dai, Dongmei Jia, Cornelia Caragea},
    booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track},
    year={2024}
}
```

## Acknowledgement
This repo borrows some data and codes from [MAVE](https://github.com/google-research-datasets/MAVE), [LaVIN](https://github.com/luogen1996/LaVIN/tree/main) and [Llama](https://github.com/meta-llama/llama). We appreciate their great works. <br>
